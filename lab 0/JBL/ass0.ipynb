{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ec5bcb6-e73e-4ac9-a84e-1c0d8ad489dc",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transform\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b3011b8-d5f2-4353-b96f-e0d95fca0265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random_split\n\u001b[0;32m      6\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      7\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m      8\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m), (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m))\n\u001b[0;32m      9\u001b[0m ])\n\u001b[1;32m---> 11\u001b[0m full_trainset \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mCIFAR10(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m     13\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.7\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(full_trainset))\n\u001b[0;32m     14\u001b[0m val_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(full_trainset) \u001b[38;5;241m-\u001b[39m train_size \n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nnlm\\Lib\\site-packages\\torchvision\\datasets\\cifar.py:68\u001b[0m, in \u001b[0;36mCIFAR10.__init__\u001b[1;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload()\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nnlm\\Lib\\site-packages\\torchvision\\datasets\\cifar.py:132\u001b[0m, in \u001b[0;36mCIFAR10._check_integrity\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename, md5 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_list \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_list:\n\u001b[0;32m    131\u001b[0m     fpath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_folder, filename)\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_integrity(fpath, md5):\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nnlm\\Lib\\site-packages\\torchvision\\datasets\\utils.py:58\u001b[0m, in \u001b[0;36mcheck_integrity\u001b[1;34m(fpath, md5)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m md5 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m check_md5(fpath, md5)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nnlm\\Lib\\site-packages\\torchvision\\datasets\\utils.py:50\u001b[0m, in \u001b[0;36mcheck_md5\u001b[1;34m(fpath, md5, **kwargs)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_md5\u001b[39m(fpath: Union[\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPath], md5: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m md5 \u001b[38;5;241m==\u001b[39m calculate_md5(fpath, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nnlm\\Lib\\site-packages\\torchvision\\datasets\\utils.py:45\u001b[0m, in \u001b[0;36mcalculate_md5\u001b[1;34m(fpath, chunk_size)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fpath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m chunk \u001b[38;5;241m:=\u001b[39m f\u001b[38;5;241m.\u001b[39mread(chunk_size):\n\u001b[1;32m---> 45\u001b[0m         md5\u001b[38;5;241m.\u001b[39mupdate(chunk)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m md5\u001b[38;5;241m.\u001b[39mhexdigest()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "train_size = int(0.7 * len(full_trainset))\n",
    "val_size = len(full_trainset) - train_size \n",
    "\n",
    "trainset, valset = random_split(full_trainset, [train_size, val_size])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Kontrollera storlekar\n",
    "print(f\"Training set: {len(trainset)}, Validation set: {len(valset)}, Test set: {len(testset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ca7729-bad1-41ae-8971-281bc93eefbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 9, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(9, 24, 3, padding = 1)\n",
    "        self.fc1 = nn.Linear(24 * 7 * 7, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cfae647-548b-4b9d-92e7-d24cff847ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.952\n",
      "[1,   400] loss: 0.962\n",
      "[2,   200] loss: 0.848\n",
      "[2,   400] loss: 0.878\n",
      "[3,   200] loss: 0.749\n",
      "[3,   400] loss: 0.806\n",
      "[4,   200] loss: 0.689\n",
      "[4,   400] loss: 0.703\n",
      "[5,   200] loss: 0.583\n",
      "[5,   400] loss: 0.629\n",
      "[6,   200] loss: 0.505\n",
      "[6,   400] loss: 0.574\n",
      "[7,   200] loss: 0.436\n",
      "[7,   400] loss: 0.503\n",
      "[8,   200] loss: 0.381\n",
      "[8,   400] loss: 0.429\n",
      "[9,   200] loss: 0.319\n",
      "[9,   400] loss: 0.381\n",
      "[10,   200] loss: 0.274\n",
      "[10,   400] loss: 0.327\n",
      "[11,   200] loss: 0.229\n",
      "[11,   400] loss: 0.273\n",
      "[12,   200] loss: 0.197\n",
      "[12,   400] loss: 0.231\n",
      "[13,   200] loss: 0.163\n",
      "[13,   400] loss: 0.222\n",
      "[14,   200] loss: 0.143\n",
      "[14,   400] loss: 0.175\n",
      "[15,   200] loss: 0.128\n",
      "[15,   400] loss: 0.159\n",
      "[16,   200] loss: 0.124\n",
      "[16,   400] loss: 0.167\n",
      "[17,   200] loss: 0.108\n",
      "[17,   400] loss: 0.139\n",
      "[18,   200] loss: 0.084\n",
      "[18,   400] loss: 0.107\n",
      "[19,   200] loss: 0.102\n",
      "[19,   400] loss: 0.128\n",
      "[20,   200] loss: 0.099\n",
      "[20,   400] loss: 0.117\n",
      "[21,   200] loss: 0.086\n",
      "[21,   400] loss: 0.104\n",
      "[22,   200] loss: 0.066\n",
      "[22,   400] loss: 0.085\n",
      "[23,   200] loss: 0.063\n",
      "[23,   400] loss: 0.097\n",
      "[24,   200] loss: 0.082\n",
      "[24,   400] loss: 0.086\n",
      "[25,   200] loss: 0.077\n",
      "[25,   400] loss: 0.091\n",
      "[26,   200] loss: 0.067\n",
      "[26,   400] loss: 0.094\n",
      "[27,   200] loss: 0.065\n",
      "[27,   400] loss: 0.102\n",
      "[28,   200] loss: 0.061\n",
      "[28,   400] loss: 0.079\n",
      "[29,   200] loss: 0.053\n",
      "[29,   400] loss: 0.070\n",
      "[30,   200] loss: 0.056\n",
      "[30,   400] loss: 0.065\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs): \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 200 == 0:  # Printa loss var 200:e batch\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fcb343f-3af5-415e-a44d-d6de12f8b1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class: plane is 66.5 %\n",
      "Accuracy for class: car   is 68.2 %\n",
      "Accuracy for class: bird  is 58.6 %\n",
      "Accuracy for class: cat   is 40.7 %\n",
      "Accuracy for class: deer  is 47.6 %\n",
      "Accuracy for class: dog   is 51.5 %\n",
      "Accuracy for class: frog  is 70.6 %\n",
      "Accuracy for class: horse is 65.8 %\n",
      "Accuracy for class: ship  is 75.7 %\n",
      "Accuracy for class: truck is 77.8 %\n",
      "Total accuracy: 62.30 %\n"
     ]
    }
   ],
   "source": [
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n",
    "\n",
    "total_correct = sum(correct_pred.values()) \n",
    "total_images = sum(total_pred.values())    \n",
    "\n",
    "total_accuracy = 100 * total_correct / total_images\n",
    "print(f'Total accuracy: {total_accuracy:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d24d7178-638f-4455-b5af-b98ba7426e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD\n",
    "#Accuracy for class: plane is 73.9 %\n",
    "#Accuracy for class: car   is 78.0 %\n",
    "#Accuracy for class: bird  is 54.1 %\n",
    "#Accuracy for class: cat   is 42.5 %\n",
    "#Accuracy for class: deer  is 61.2 %\n",
    "#Accuracy for class: dog   is 49.3 %\n",
    "#Accuracy for class: frog  is 72.1 %\n",
    "#Accuracy for class: horse is 77.0 %\n",
    "#Accuracy for class: ship  is 71.1 %\n",
    "#Accuracy for class: truck is 71.0 %\n",
    "#Total accuracy: 65.02 %\n",
    "\n",
    "# ADAM\n",
    "#Accuracy for class: plane is 66.5 %\n",
    "#Accuracy for class: car   is 68.2 %\n",
    "#Accuracy for class: bird  is 58.6 %\n",
    "#Accuracy for class: cat   is 40.7 %\n",
    "#Accuracy for class: deer  is 47.6 %\n",
    "#Accuracy for class: dog   is 51.5 %\n",
    "#Accuracy for class: frog  is 70.6 %\n",
    "#Accuracy for class: horse is 65.8 %\n",
    "#Accuracy for class: ship  is 75.7 %\n",
    "#Accuracy for class: truck is 77.8 %\n",
    "#Total accuracy: 62.30 %\n",
    "\n",
    "# KONSTIGT DÅ LOSS VAR BETYDLIGT LÄGRE ; OVERFITTING?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dbbf66-5d9a-4bde-aa35-5fbae1f92c22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c5536d-d9a0-442d-9759-ff67e28785d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
